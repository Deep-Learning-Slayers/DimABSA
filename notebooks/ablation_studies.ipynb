{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.data_loader import load_dimabsa_dataset, create_dataloaders\n",
    "from src.models.deberta_dimabsa import DeBERTaDimABSA, DeBERTaDimABSAConfig\n",
    "from src.models.baselines import AspectMeanPoolingModel, TransformerVARegressor\n",
    "from src.trainer import TrainingConfig, train_model\n",
    "from src.lexicon import create_lexicon\n",
    "\n",
    "# Configs\n",
    "DATA_DIR = PROJECT_ROOT / 'DimABSA2026' / 'task-dataset' / 'track_a' / 'subtask_1'\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs' / 'ablation'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load datasets\n",
    "raw = load_dimabsa_dataset(DATA_DIR, lang='eng', domain='laptop', split_dev=True)\n",
    "train_df = raw['train']\n",
    "dev_df = raw['dev']\n",
    "print(f'Train: {len(train_df)}, Dev: {len(dev_df)}')\n",
    "\n",
    "#Lexicon extractor\n",
    "try:\n",
    "    lexicon = create_lexicon(PROJECT_ROOT / 'NRC-VAD-Lexicon-v2.1', use_dependency_parsing=False)\n",
    "    lexicon_extractor = lambda texts, aspects: lexicon.extract_batch_features(texts, aspects)\n",
    "    print('Lexicon loaded')\n",
    "except Exception as e:\n",
    "    print('Lexicon not available:', e)\n",
    "    def lexicon_extractor(texts, aspects):\n",
    "        return torch.zeros((len(texts), 8), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce552208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample train for fast experiments\n",
    "USE_SUBSET = True\n",
    "SUBSET_FRACTION = 0.2\n",
    "SUBSET_MAX = 1500\n",
    "if USE_SUBSET:\n",
    "    n = min(len(train_df), max(1, int(len(train_df) * SUBSET_FRACTION)), SUBSET_MAX)\n",
    "    train_df_small = train_df.sample(n=n, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    train_df_small = train_df\n",
    "print('Using train subset size =', len(train_df_small))\n",
    "\n",
    "# Ablation settings\n",
    "scalings = ['sigmoid', 'tanh', 'linear']\n",
    "poolings = ['attention', 'mean', 'cls']\n",
    "lexicon_flags = [True, False]\n",
    "\n",
    "# Build experiments list (simple Cartesian product)\n",
    "ablation_exps = list(itertools.product(scalings, poolings, lexicon_flags))\n",
    "len(ablation_exps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runner for ablation experiments (not executed automatically)\n",
    "results = []\n",
    "for i, (scaling, pooling, use_lex) in enumerate(ablation_exps, 1):\n",
    "    print(f'Experiment {i}/{len(ablation_exps)} - scaling={scaling}, pooling={pooling}, lexicon={use_lex}')\n",
    "\n",
    "    # choose model class and kwargs\n",
    "    if pooling == 'attention':\n",
    "        cfg = DeBERTaDimABSAConfig(model_name=MODEL_NAME, use_lexicon=use_lex, lexicon_feature_dim=8, output_scaling=scaling)\n",
    "        model = DeBERTaDimABSA(cfg)\n",
    "    elif pooling == 'mean':\n",
    "        model = AspectMeanPoolingModel(model_name=MODEL_NAME, output_scaling=scaling)\n",
    "    else:\n",
    "        model = TransformerVARegressor(model_name=MODEL_NAME, pooling='cls', output_scaling=scaling)\n",
    "\n",
    "    # dataloaders (small subset)\n",
    "    train_loader, dev_loader = create_dataloaders(train_df_small, dev_df, tokenizer, batch_size=16, max_length=128, use_aspect_aware=True)\n",
    "\n",
    "    cfg_train = TrainingConfig(learning_rate=2e-5, num_epochs=3, checkpoint_dir=str(OUTPUT_DIR / 'checkpoints'), device=str(DEVICE))\n",
    "\n",
    "    # run training (uncomment to execute)\n",
    "    res = train_model(model, train_loader, dev_loader, lexicon_extractor=lexicon_extractor if use_lex else None, config=cfg_train, model_name=f'ablation_{i}')\n",
    "    results.append({'scaling': scaling, 'pooling': pooling, 'use_lex': use_lex, 'best_metric': res['best_metric']})\n",
    "\n",
    "# Save planned experiments\n",
    "import pandas as pd\n",
    "pd.DataFrame([{'scaling': s, 'pooling': p, 'use_lex': l} for s,p,l in ablation_exps]).to_csv(OUTPUT_DIR / 'ablation_plan.csv', index=False)\n",
    "print('Ablation plan saved to', OUTPUT_DIR / 'ablation_plan.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f5662f",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "The cells below create three visualizations if `outputs/ablation/ablation_results.csv` exists: grouped barplot (scaling × pooling, hue=lexicon), violin plot (distribution by pooling), and heatmap (scaling vs pooling mean metric when lexicon on).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d872eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path.cwd().parent / 'outputs' / 'ablation'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_path = OUT_DIR / 'ablation_results.csv'\n",
    "if not results_path.exists():\n",
    "    print('No ablation results file found at', results_path)\n",
    "    print('Run the ablation experiments to produce', results_path)\n",
    "else:\n",
    "    df = pd.read_csv(results_path)\n",
    "    # Ensure columns exist: scaling, pooling, use_lex, best_metric\n",
    "    if not {'scaling','pooling','use_lex','best_metric'}.issubset(df.columns):\n",
    "        print('Expected columns missing in', results_path)\n",
    "    else:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.catplot(data=df, x='scaling', y='best_metric', hue='pooling', kind='bar', ci='sd', height=5, aspect=1.5)\n",
    "        plt.title('Mean Best Metric by Output Scaling and Pooling')\n",
    "        plt.tight_layout()\n",
    "        out_file = OUT_DIR / 'ablation_barplot_scaling_pooling.png'\n",
    "        plt.savefig(out_file, dpi=200)\n",
    "        plt.show()\n",
    "        print('Saved', out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot: distribution of best_metric by pooling (hue=scaling)\n",
    "import numpy as np\n",
    "\n",
    "results_path = OUT_DIR / 'ablation_results.csv'\n",
    "if not results_path.exists():\n",
    "    print('No ablation results file found at', results_path)\n",
    "else:\n",
    "    df = pd.read_csv(results_path)\n",
    "    if 'pooling' not in df.columns or 'best_metric' not in df.columns:\n",
    "        print('Expected columns missing in', results_path)\n",
    "    else:\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.violinplot(data=df, x='pooling', y='best_metric', hue='scaling', split=True, inner='quartile')\n",
    "        plt.title('Distribution of Best Metric by Pooling (colored by scaling)')\n",
    "        plt.tight_layout()\n",
    "        out_file = OUT_DIR / 'ablation_violin_pooling.png'\n",
    "        plt.savefig(out_file, dpi=200)\n",
    "        plt.show()\n",
    "        print('Saved', out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: scaling × pooling mean metric when lexicon is True\n",
    "results_path = OUT_DIR / 'ablation_results.csv'\n",
    "if not results_path.exists():\n",
    "    print('No ablation results file found at', results_path)\n",
    "else:\n",
    "    df = pd.read_csv(results_path)\n",
    "    # focus on lexicon-on experiments for clearer pivot\n",
    "    df_on = df[df['use_lex'] == True] if 'use_lex' in df.columns else df\n",
    "    if df_on.empty:\n",
    "        print('No lexicon-on rows found; using all rows for heatmap')\n",
    "        df_on = df\n",
    "    try:\n",
    "        pivot = df_on.pivot_table(index='pooling', columns='scaling', values='best_metric', aggfunc='mean')\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.heatmap(pivot, annot=True, fmt='.4f', cmap='vlag')\n",
    "        plt.title('Mean Best Metric (lexicon-on) — Pooling vs Scaling')\n",
    "        plt.tight_layout()\n",
    "        out_file = OUT_DIR / 'ablation_heatmap_pooling_scaling.png'\n",
    "        plt.savefig(out_file, dpi=200)\n",
    "        plt.show()\n",
    "        print('Saved', out_file)\n",
    "    except Exception as e:\n",
    "        print('Could not create heatmap:', e)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
