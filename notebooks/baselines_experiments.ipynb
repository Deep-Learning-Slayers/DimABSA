{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed26476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from src.data_loader import load_dimabsa_dataset, create_dataloaders\n",
    "from src.models.baselines import BERTDimABSA, RoBERTaDimABSA\n",
    "from src.models.deberta_dimabsa import DeBERTaDimABSA, DeBERTaDimABSAConfig\n",
    "from src.trainer import TrainingConfig, train_model\n",
    "from src.lexicon import create_lexicon\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'DimABSA2026' / 'task-dataset' / 'track_a' / 'subtask_1'\n",
    "MODEL_NAMES = {'bert': 'bert-base-uncased', 'roberta': 'roberta-base', 'deberta': 'microsoft/deberta-v3-base'}\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs' / 'baselines'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "raw = load_dimabsa_dataset(DATA_DIR, lang='eng', domain='laptop', split_dev=True)\n",
    "train_df = raw['train']\n",
    "dev_df = raw['dev']\n",
    "\n",
    "# Create a single lexicon extractor used for all baselines (fallback to zeros if lexicon missing)\n",
    "try:\n",
    "    lex = create_lexicon(PROJECT_ROOT / 'NRC-VAD-Lexicon-v2.1', use_dependency_parsing=False)\n",
    "    lex_extractor = lambda texts, aspects: lex.extract_batch_features(texts, aspects)\n",
    "    print('Lexicon loaded for baselines')\n",
    "except Exception as e:\n",
    "    print('Lexicon unavailable, using zero-features fallback:', e)\n",
    "    lex_extractor = lambda texts, aspects: torch.zeros((len(texts), 8), dtype=torch.float32)\n",
    "\n",
    "results = []\n",
    "for name, model_name in MODEL_NAMES.items():\n",
    "    print('Preparing baseline:', name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if name == 'bert':\n",
    "        model = BERTDimABSA(model_name=model_name)\n",
    "    elif name == 'roberta':\n",
    "        model = RoBERTaDimABSA(model_name=model_name)\n",
    "    else:\n",
    "        cfg = DeBERTaDimABSAConfig(model_name=model_name, use_lexicon=True, lexicon_feature_dim=8, output_scaling='tanh')\n",
    "        model = DeBERTaDimABSA(cfg)\n",
    "\n",
    "    train_loader, dev_loader = create_dataloaders(\n",
    "        train_df.sample(n=min(len(train_df), 1000), random_state=42),\n",
    "        dev_df,\n",
    "        tokenizer,\n",
    "        batch_size=16,\n",
    "        max_length=128,\n",
    "        use_aspect_aware=True\n",
    "    )\n",
    "\n",
    "    cfg = TrainingConfig(learning_rate=2e-5, num_epochs=3, checkpoint_dir=str(OUTPUT_DIR / 'checkpoints'), device=str(DEVICE))\n",
    "    #lex_extractor is passed for all models so each uses lexicon features.\n",
    "    res = train_model(model, train_loader, dev_loader, lexicon_extractor=lex_extractor, config=cfg, model_name=f'baseline_{name}')\n",
    "    results.append({'model': name, 'best_metric': res['best_metric']})\n",
    "\n",
    "# Save plan\n",
    "pd.DataFrame([{'model': k, 'model_name': v} for k, v in MODEL_NAMES.items()]).to_csv(OUTPUT_DIR / 'baselines_plan.csv', index=False)\n",
    "print('Baselines plan saved to', OUTPUT_DIR / 'baselines_plan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path.cwd().parent / 'outputs' / 'baselines'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "results_path = OUT_DIR / 'baselines_results.csv'\n",
    "if not results_path.exists():\n",
    "    print('No baselines results file found at', results_path)\n",
    "    print('Run baseline experiments to produce', results_path)\n",
    "else:\n",
    "    df = pd.read_csv(results_path)\n",
    "    if not {'model','best_metric'}.issubset(df.columns):\n",
    "        print('Expected columns missing in', results_path)\n",
    "    else:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.barplot(data=df, x='model', y='best_metric', ci='sd', palette='muted')\n",
    "        plt.title('Baseline Models â€” Best Metric (lower is better)')\n",
    "        plt.tight_layout()\n",
    "        out_file = OUT_DIR / 'baselines_barplot.png'\n",
    "        plt.savefig(out_file, dpi=200)\n",
    "        plt.show()\n",
    "        print('Saved', out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of baseline results\n",
    "results_path = OUT_DIR / 'baselines_results.csv'\n",
    "if not results_path.exists():\n",
    "    print('No baselines results file found at', results_path)\n",
    "else:\n",
    "    df = pd.read_csv(results_path)\n",
    "    if not {'model','best_metric'}.issubset(df.columns):\n",
    "        print('Expected columns missing in', results_path)\n",
    "    else:\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.boxplot(data=df, x='model', y='best_metric', palette='pastel')\n",
    "        sns.swarmplot(data=df, x='model', y='best_metric', color='k', alpha=0.6)\n",
    "        plt.title('Baseline Metric Distributions')\n",
    "        plt.tight_layout()\n",
    "        out_file = OUT_DIR / 'baselines_boxplot.png'\n",
    "        plt.savefig(out_file, dpi=200)\n",
    "        plt.show()\n",
    "        print('Saved', out_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
